{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["9_J-ezzpiP5d","Mz5YZP77quCj"],"toc_visible":true,"authorship_tag":"ABX9TyMxx22BDIzwjz3EmZrgpbRs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Lab 08-1: Perceptron & Lab 08-2: Multi Layer Perceptron"],"metadata":{"id":"Do3ZUBB3iL4K"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"rTDyz30JNqEZ","executionInfo":{"status":"ok","timestamp":1695982911301,"user_tz":-540,"elapsed":404,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# for reproducibility\n","torch.manual_seed(777)\n","if device == 'cuda':\n","    torch.cuda.manual_seed_all(777)"],"metadata":{"id":"G4oUervliR2j","executionInfo":{"status":"ok","timestamp":1695982911301,"user_tz":-540,"elapsed":2,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Backpropagation 구현"],"metadata":{"id":"dGobTpkmjo8R"}},{"cell_type":"code","source":["X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n","Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)"],"metadata":{"id":"LoE1MKpBiSQa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nn Layers\n","w1 = torch.Tensor(2, 2).to(device)\n","b1 = torch.Tensor(2).to(device)\n","w2 = torch.Tensor(2, 1).to(device)\n","b2 = torch.Tensor(1).to(device)"],"metadata":{"id":"DtpsYPjWjvEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sigmoid(x):\n","    return 1.0/(1.0 + torch.exp(-x))\n","\n","def sigmoid_prime(x):\n","    return sigmoid(x) * (1- sigmoid(x))"],"metadata":{"id":"MuzG75dXjvI0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = 0.000001\n","\n","# 학습\n","for epoch in range(10001):\n","    # forward 연산\n","    l1 = torch.add(torch.matmul(X, w1), b1)\n","    a1 = sigmoid(l1)\n","    l2 = torch.add(torch.matmul(a1, w2), b2)\n","    y_pred = sigmoid(l2)\n","\n","    # 비용 함수\n","    cost = -torch.mean(Y*torch.log(y_pred) + (1-Y)*torch.log(1-y_pred))\n","\n","    # Backpropagation\n","    d_y_pred = (y_pred - Y) # / (y_pred * (1.0 - y_pred) + 1e-7)\n","\n","    d_l2 = d_y_pred * sigmoid_prime(l2)\n","    d_b2 = d_l2\n","    d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)\n","\n","    d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n","    d_l1 = d_a1 * sigmoid_prime(l1)\n","    d_b1 = d_l1\n","    d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)\n","\n","    # update\n","    w1 = w1 - learning_rate * d_w1\n","    b1 = b1 - learning_rate * torch.mean(d_b1, 0)\n","    w2 = w2 - learning_rate * d_w2\n","    b1 = b1 - learning_rate * torch.mean(d_b2, 0)\n","\n","    # 100의 배수에 해당되는 에포크마다 비용을 출력\n","    if epoch % 1000 == 0:\n","        print(epoch, cost.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jym1-53jjvNT","executionInfo":{"status":"ok","timestamp":1695885637025,"user_tz":-540,"elapsed":4161,"user":{"displayName":"고은경","userId":"10765015356207281747"}},"outputId":"4df1eee2-e16d-4c90-d136-08b23a2dbd97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 nan\n","1000 nan\n","2000 nan\n","3000 nan\n","4000 nan\n","5000 nan\n","6000 nan\n","7000 nan\n","8000 nan\n","9000 nan\n","10000 nan\n"]}]},{"cell_type":"markdown","source":["## XOR 문제 - 다층 퍼셉트론 구현하기"],"metadata":{"id":"9_J-ezzpiP5d"}},{"cell_type":"code","source":["X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n","Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)"],"metadata":{"id":"9QzDbLHWiR7P","executionInfo":{"status":"ok","timestamp":1695985261606,"user_tz":-540,"elapsed":3,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# 은닉층이 3개인 인공 신경망\n","model = nn.Sequential(\n","          nn.Linear(2, 10, bias=True), # input_layer = 2, hidden_layer1 = 10\n","          nn.Sigmoid(),\n","          nn.Linear(10, 10, bias=True), # hidden_layer1 = 10, hidden_layer2 = 10\n","          nn.Sigmoid(),\n","          nn.Linear(10, 10, bias=True), # hidden_layer2 = 10, hidden_layer3 = 10\n","          nn.Sigmoid(),\n","          nn.Linear(10, 1, bias=True), # hidden_layer3 = 10, output_layer = 1\n","          nn.Sigmoid()\n","          ).to(device)"],"metadata":{"id":"9FUlfUkNiSAC","executionInfo":{"status":"ok","timestamp":1695985282301,"user_tz":-540,"elapsed":317,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# 비용 함수와 옵타마이저 선언\n","criterion = torch.nn.BCELoss().to(device) # binary cross entrophy\n","optimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1"],"metadata":{"id":"sbk5cR1-iSEt","executionInfo":{"status":"ok","timestamp":1695985358752,"user_tz":-540,"elapsed":2,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# 학습\n","for epoch in range(10001):\n","    optimizer.zero_grad()\n","\n","    # forward 연산\n","    hypothesis = model(X)\n","\n","    # 비용 함수\n","    cost = criterion(hypothesis, Y)\n","    cost.backward()\n","    optimizer.step()\n","\n","    if epoch % 1000 == 0:\n","        print(epoch, cost.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8XQzV41giSIs","executionInfo":{"status":"ok","timestamp":1695883665105,"user_tz":-540,"elapsed":7214,"user":{"displayName":"고은경","userId":"10765015356207281747"}},"outputId":"ce74a3b7-486d-4557-baef-bb57385b38d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.7408947944641113\n","1000 0.6931225061416626\n","2000 0.6930870413780212\n","3000 0.6929655075073242\n","4000 0.6907041668891907\n","5000 0.001293981447815895\n","6000 0.00044235470704734325\n","7000 0.00026008012355305254\n","8000 0.0001824370992835611\n","9000 0.00013982132077217102\n","10000 0.00011304025974823162\n"]}]},{"cell_type":"code","source":["# 테스트\n","# with torch.no_grad(): autograd를 끔으로써 메모리 사용량을 줄이고 연산 속도를 높히는 역할\n","with torch.no_grad():\n","    hypothesis = model(X)\n","    predicted = (hypothesis > 0.5).float() # true -> 1, false -> 0\n","    accuracy = (predicted == Y).float().mean()\n","    print('모델의 출력값(Hypothesis): ', hypothesis.detach().cpu().numpy())\n","    print('모델의 예측값(Predicted): ', predicted.detach().cpu().numpy())\n","    print('실제값(Y): ', Y.cpu().numpy())\n","    print('정확도(Accuracy): ', accuracy.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K8AUG_AqiSMe","executionInfo":{"status":"ok","timestamp":1695883675910,"user_tz":-540,"elapsed":431,"user":{"displayName":"고은경","userId":"10765015356207281747"}},"outputId":"60a02adc-f224-49ce-d390-f5555b1f69c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["모델의 출력값(Hypothesis):  [[6.8877263e-05]\n"," [9.9988139e-01]\n"," [9.9989223e-01]\n"," [1.5683484e-04]]\n","모델의 예측값(Predicted):  [[0.]\n"," [1.]\n"," [1.]\n"," [0.]]\n","실제값(Y):  [[0.]\n"," [1.]\n"," [1.]\n"," [0.]]\n","정확도(Accuracy):  1.0\n"]}]},{"cell_type":"markdown","source":["# Lab 09-1: ReLU"],"metadata":{"id":"Mz5YZP77quCj"}},{"cell_type":"code","source":["import torch\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","import random\n","import matplotlib.pylab as plt"],"metadata":{"id":"aWgrdc5_qr-b","executionInfo":{"status":"ok","timestamp":1695983243086,"user_tz":-540,"elapsed":339,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# parameters\n","learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 100"],"metadata":{"id":"Py6P-m7u18Il"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MNIST dataset\n","mnist_train = dsets.MNIST(root='MNIST_data/',\n","                          train=True,\n","                          transform=transforms.ToTensor(),\n","                          download=True)\n","\n","mnist_test = dsets.MNIST(root='MNIST_data/',\n","                         train=False,\n","                         transform=transforms.ToTensor(),\n","                         download=True)"],"metadata":{"id":"WX-sjutK18NY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset loader\n","data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n","                                          batch_size=batch_size,\n","                                          shuffle=True,\n","                                          drop_last=True)"],"metadata":{"id":"rR-rA2eA18Rh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nn layers\n","linear1 = torch.nn.Linear(784, 512, bias=True)\n","linear2 = torch.nn.Linear(512, 512, bias=True)\n","linear3 = torch.nn.Linear(512, 512, bias=True)\n","linear4 = torch.nn.Linear(512, 512, bias=True)\n","linear5 = torch.nn.Linear(512, 10, bias=True)\n","relu = torch.nn.ReLU()\n","\n","# model\n","model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)\n","\n","# define cost/loss & optimizer\n","criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"CGgKuVOz18Vr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train\n","total_batch = len(data_loader)\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","\n","    for X, Y in data_loader:\n","        # reshape input image into [batch_size by 784]\n","        # label is not one-hot encoded\n","        X = X.view(-1, 28 * 28).to(device)\n","        Y = Y.to(device)\n","\n","        optimizer.zero_grad()\n","        hypothesis = model(X)\n","        cost = criterion(hypothesis, Y)\n","        cost.backward()\n","        optimizer.step()\n","\n","        avg_cost += cost / total_batch\n","    if (epoch+1) % 5 == 0:\n","        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning finished')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VS3WQm4718ZR","executionInfo":{"status":"ok","timestamp":1695889275205,"user_tz":-540,"elapsed":337042,"user":{"displayName":"고은경","userId":"10765015356207281747"}},"outputId":"46c161b7-f428-443a-8d2b-2303fc0ec91c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0001 cost = 0.358217984\n","Epoch: 0006 cost = 0.029212100\n","Epoch: 0011 cost = 0.015273080\n","Learning finished\n"]}]},{"cell_type":"code","source":["# Test the model using test sets\n","with torch.no_grad():\n","    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n","    Y_test = mnist_test.test_labels.to(device)\n","\n","    prediction = model(X_test)\n","    correct_prediction = torch.argmax(prediction, 1) == Y_test\n","    accuracy = correct_prediction.float().mean()\n","    print('Accuracy:', accuracy.item())\n","\n","    # Get one and predict\n","    r = random.randint(0, len(mnist_test) - 1)\n","    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n","    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n","\n","    print('Label: ', Y_single_data.item())\n","    single_prediction = model(X_single_data)\n","    print('Prediction: ', torch.argmax(single_prediction, 1).item())"],"metadata":{"id":"_fnEZD_bqtB1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Lab 09-2: Weight initialization"],"metadata":{"id":"5aQ_ev4c3Pw7"}},{"cell_type":"code","source":["# parameters\n","learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 100"],"metadata":{"id":"wSrEK7Z3qtHb","executionInfo":{"status":"ok","timestamp":1695982921550,"user_tz":-540,"elapsed":309,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# MNIST dataset\n","mnist_train = dsets.MNIST(root='MNIST_data/',\n","                          train=True,\n","                          transform=transforms.ToTensor(),\n","                          download=True)\n","\n","mnist_test = dsets.MNIST(root='MNIST_data/',\n","                         train=False,\n","                         transform=transforms.ToTensor(),\n","                         download=True)"],"metadata":{"id":"uaC-cmHqqtLv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset loader\n","data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n","                                          batch_size=batch_size,\n","                                          shuffle=True,\n","                                          drop_last=True)"],"metadata":{"id":"KHnfMsfkqtP3","executionInfo":{"status":"ok","timestamp":1695982945372,"user_tz":-540,"elapsed":363,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# nn layers\n","linear1 = torch.nn.Linear(784, 256, bias=True)\n","linear2 = torch.nn.Linear(256, 256, bias=True)\n","linear3 = torch.nn.Linear(256, 10, bias=True)\n","relu = torch.nn.ReLU()\n","\n","# xavier initialization\n","torch.nn.init.xavier_uniform_(linear1.weight)\n","torch.nn.init.xavier_uniform_(linear2.weight)\n","torch.nn.init.xavier_uniform_(linear3.weight)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LQtGcNbldOTG","executionInfo":{"status":"ok","timestamp":1695982956201,"user_tz":-540,"elapsed":3,"user":{"displayName":"고은경","userId":"10765015356207281747"}},"outputId":"fe677835-0c5d-41c5-8a15-7f7b436471c8"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[-0.0215, -0.0894,  0.0598,  ...,  0.0200,  0.0203,  0.1212],\n","        [ 0.0078,  0.1378,  0.0920,  ...,  0.0975,  0.1458, -0.0302],\n","        [ 0.1270, -0.1296,  0.1049,  ...,  0.0124,  0.1173, -0.0901],\n","        ...,\n","        [ 0.0661, -0.1025,  0.1437,  ...,  0.0784,  0.0977, -0.0396],\n","        [ 0.0430, -0.1274, -0.0134,  ..., -0.0582,  0.1201,  0.1479],\n","        [-0.1433,  0.0200, -0.0568,  ...,  0.0787,  0.0428, -0.0036]],\n","       requires_grad=True)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# model\n","model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)\n","\n","# define cost/loss & optimizer\n","criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"IUguXhQMdjY4","executionInfo":{"status":"ok","timestamp":1695982978147,"user_tz":-540,"elapsed":3,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["total_batch = len(data_loader)\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","\n","    for X, Y in data_loader:\n","        # reshape input image into [batch_size by 784]\n","        # label is not one-hot encoded\n","        X = X.view(-1, 28 * 28).to(device)\n","        Y = Y.to(device)\n","\n","        optimizer.zero_grad()\n","        hypothesis = model(X)\n","        cost = criterion(hypothesis, Y)\n","        cost.backward()\n","        optimizer.step()\n","\n","        avg_cost += cost / total_batch\n","\n","    if (epoch+1) % 5 == 0:\n","        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning finished')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p23zZm3idjdh","executionInfo":{"status":"ok","timestamp":1695983199686,"user_tz":-540,"elapsed":177917,"user":{"displayName":"고은경","userId":"10765015356207281747"}},"outputId":"cf34884c-15e3-4966-c732-570fd3b81bcd"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0005 cost = 0.032775719\n","Epoch: 0010 cost = 0.013457117\n","Epoch: 0015 cost = 0.008104536\n","Learning finished\n"]}]},{"cell_type":"code","source":["# Test the model using test sets\n","with torch.no_grad():\n","    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n","    Y_test = mnist_test.test_labels.to(device)\n","\n","    prediction = model(X_test)\n","    correct_prediction = torch.argmax(prediction, 1) == Y_test\n","    accuracy = correct_prediction.float().mean()\n","    print('Accuracy:', accuracy.item())\n","\n","    # Get one and predict\n","    r = random.randint(0, len(mnist_test) - 1)\n","    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n","    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n","\n","    print('Label: ', Y_single_data.item())\n","    single_prediction = model(X_single_data)\n","    print('Prediction: ', torch.argmax(single_prediction, 1).item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LK5yYWVLdjhp","executionInfo":{"status":"ok","timestamp":1695983252667,"user_tz":-540,"elapsed":465,"user":{"displayName":"고은경","userId":"10765015356207281747"}},"outputId":"79bb98e1-c94e-4ec7-d359-d69b7fb04e5c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n","  warnings.warn(\"test_data has been renamed data\")\n","/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n","  warnings.warn(\"test_labels has been renamed targets\")\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9765999913215637\n","Label:  7\n","Prediction:  7\n"]}]},{"cell_type":"markdown","source":["# Lab 09-3: Dropout"],"metadata":{"id":"_hr_L0GFdHqc"}},{"cell_type":"code","source":["# parameters\n","learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 100\n","drop_prob = 0.3"],"metadata":{"id":"eoFtIQw9dJ-Q","executionInfo":{"status":"ok","timestamp":1695983255319,"user_tz":-540,"elapsed":2,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# nn layers\n","linear1 = torch.nn.Linear(784, 512, bias=True)\n","linear2 = torch.nn.Linear(512, 512, bias=True)\n","linear3 = torch.nn.Linear(512, 512, bias=True)\n","linear4 = torch.nn.Linear(512, 512, bias=True)\n","linear5 = torch.nn.Linear(512, 10, bias=True)\n","relu = torch.nn.ReLU()\n","dropout = torch.nn.Dropout(p=drop_prob)\n","\n","# xavier initialization\n","torch.nn.init.xavier_uniform_(linear1.weight)\n","torch.nn.init.xavier_uniform_(linear2.weight)\n","torch.nn.init.xavier_uniform_(linear3.weight)\n","torch.nn.init.xavier_uniform_(linear4.weight)\n","torch.nn.init.xavier_uniform_(linear5.weight)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sg9ukaLJdKsh","executionInfo":{"status":"ok","timestamp":1695983256873,"user_tz":-540,"elapsed":2,"user":{"displayName":"고은경","userId":"10765015356207281747"}},"outputId":"6c6108dc-75a3-4a1f-cb90-f74cfe143481"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[ 0.0579, -0.1055,  0.0272,  ...,  0.1036,  0.0291, -0.0746],\n","        [ 0.0528,  0.0494, -0.0059,  ..., -0.0951, -0.0887, -0.0129],\n","        [-0.0953, -0.0510,  0.1030,  ..., -0.0493, -0.0581, -0.0280],\n","        ...,\n","        [ 0.0631,  0.0917,  0.0690,  ..., -0.0746, -0.0798,  0.0231],\n","        [ 0.0937,  0.0521, -0.0346,  ...,  0.0432, -0.0982, -0.0107],\n","        [-0.0888, -0.0723,  0.0511,  ...,  0.0530, -0.0830,  0.0624]],\n","       requires_grad=True)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# model\n","model = torch.nn.Sequential(linear1, relu, dropout,\n","                            linear2, relu, dropout,\n","                            linear3, relu, dropout,\n","                            linear4, relu, dropout,\n","                            linear5).to(device)\n","\n","# define cost/loss & optimizer\n","criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"KyBzLOSJdKw7","executionInfo":{"status":"ok","timestamp":1695983259184,"user_tz":-540,"elapsed":280,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["total_batch = len(data_loader)\n","model.train()    # set the model to train mode (dropout=True)\n","\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","\n","    for X, Y in data_loader:\n","        # reshape input image into [batch_size by 784]\n","        # label is not one-hot encoded\n","        X = X.view(-1, 28 * 28).to(device)\n","        Y = Y.to(device)\n","\n","        optimizer.zero_grad()\n","        hypothesis = model(X)\n","        cost = criterion(hypothesis, Y)\n","        cost.backward()\n","        optimizer.step()\n","\n","        avg_cost += cost / total_batch\n","\n","    if (epoch+1) % 5 == 0:\n","        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning finished')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nip359E4dK1Y","executionInfo":{"status":"ok","timestamp":1695983603992,"user_tz":-540,"elapsed":343314,"user":{"displayName":"고은경","userId":"10765015356207281747"}},"outputId":"e6ff3350-7aaf-4dd7-c922-7e7648620477"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0005 cost = 0.083198912\n","Epoch: 0010 cost = 0.056024872\n","Epoch: 0015 cost = 0.042520519\n","Learning finished\n"]}]},{"cell_type":"code","source":["# Test model and check accuracy\n","with torch.no_grad():\n","    model.eval()    # set the model to evaluation mode (dropout=False)\n","\n","    # Test the model using test sets\n","    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n","    Y_test = mnist_test.test_labels.to(device)\n","\n","    prediction = model(X_test)\n","    correct_prediction = torch.argmax(prediction, 1) == Y_test\n","    accuracy = correct_prediction.float().mean()\n","    print('Accuracy:', accuracy.item())\n","\n","    # Get one and predict\n","    r = random.randint(0, len(mnist_test) - 1)\n","    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n","    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n","\n","    print('Label: ', Y_single_data.item())\n","    single_prediction = model(X_single_data)\n","    print('Prediction: ', torch.argmax(single_prediction, 1).item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J6mJINEEeLkO","executionInfo":{"status":"ok","timestamp":1695985270214,"user_tz":-540,"elapsed":589,"user":{"displayName":"고은경","userId":"10765015356207281747"}},"outputId":"927e2faa-c4d8-4150-c7ae-81deca2a2194"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9782999753952026\n","Label:  2\n","Prediction:  2\n"]}]},{"cell_type":"markdown","source":["# Lab 09-4: Batch Normalization"],"metadata":{"id":"k6PYrbpFdLSb"}},{"cell_type":"code","source":["# parameters\n","learning_rate = 0.01\n","training_epochs = 10\n","batch_size = 32"],"metadata":{"id":"p8yMlU2GdK5d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset loader\n","train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n","                                          batch_size=batch_size,\n","                                          shuffle=True,\n","                                          drop_last=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n","                                          batch_size=batch_size,\n","                                          shuffle=False,\n","                                          drop_last=True)"],"metadata":{"id":"3lgzJwoPezTv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nn layers\n","linear1 = torch.nn.Linear(784, 32, bias=True)\n","linear2 = torch.nn.Linear(32, 32, bias=True)\n","linear3 = torch.nn.Linear(32, 10, bias=True)\n","relu = torch.nn.ReLU()\n","bn1 = torch.nn.BatchNorm1d(32)\n","bn2 = torch.nn.BatchNorm1d(32)\n","\n","nn_linear1 = torch.nn.Linear(784, 32, bias=True)\n","nn_linear2 = torch.nn.Linear(32, 32, bias=True)\n","nn_linear3 = torch.nn.Linear(32, 10, bias=True)"],"metadata":{"id":"HwU1vKIxdNbB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model\n","bn_model = torch.nn.Sequential(linear1, bn1, relu,\n","                            linear2, bn2, relu,\n","                            linear3).to(device)\n","nn_model = torch.nn.Sequential(nn_linear1, relu,\n","                               nn_linear2, relu,\n","                               nn_linear3).to(device)\n","\n","# define cost/loss & optimizer\n","criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n","bn_optimizer = torch.optim.Adam(bn_model.parameters(), lr=learning_rate)\n","nn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)"],"metadata":{"id":"22uvwJNgdNe8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save Losses and Accuracies every epoch\n","# We are going to plot them later\n","train_losses = []\n","train_accs = []\n","\n","valid_losses = []\n","valid_accs = []\n","\n","train_total_batch = len(train_loader)\n","test_total_batch = len(test_loader)\n","for epoch in range(training_epochs):\n","    bn_model.train()  # set the model to train mode\n","\n","    for X, Y in train_loader:\n","        # reshape input image into [batch_size by 784]\n","        # label is not one-hot encoded\n","        X = X.view(-1, 28 * 28).to(device)\n","        Y = Y.to(device)\n","\n","        bn_optimizer.zero_grad()\n","        bn_prediction = bn_model(X)\n","        bn_loss = criterion(bn_prediction, Y)\n","        bn_loss.backward()\n","        bn_optimizer.step()\n","\n","        nn_optimizer.zero_grad()\n","        nn_prediction = nn_model(X)\n","        nn_loss = criterion(nn_prediction, Y)\n","        nn_loss.backward()\n","        nn_optimizer.step()\n","\n","    with torch.no_grad():\n","        bn_model.eval()     # set the model to evaluation mode\n","\n","        # Test the model using train sets\n","        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n","        for i, (X, Y) in enumerate(train_loader):\n","            X = X.view(-1, 28 * 28).to(device)\n","            Y = Y.to(device)\n","\n","            bn_prediction = bn_model(X)\n","            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n","            bn_loss += criterion(bn_prediction, Y)\n","            bn_acc += bn_correct_prediction.float().mean()\n","\n","            nn_prediction = nn_model(X)\n","            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n","            nn_loss += criterion(nn_prediction, Y)\n","            nn_acc += nn_correct_prediction.float().mean()\n","\n","        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / train_total_batch, nn_loss / train_total_batch, bn_acc / train_total_batch, nn_acc / train_total_batch\n","\n","        # Save train losses/acc\n","        train_losses.append([bn_loss, nn_loss])\n","        train_accs.append([bn_acc, nn_acc])\n","        print(\n","            '[Epoch %d-TRAIN] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n","            (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\n","        # Test the model using test sets\n","        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n","        for i, (X, Y) in enumerate(test_loader):\n","            X = X.view(-1, 28 * 28).to(device)\n","            Y = Y.to(device)\n","\n","            bn_prediction = bn_model(X)\n","            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n","            bn_loss += criterion(bn_prediction, Y)\n","            bn_acc += bn_correct_prediction.float().mean()\n","\n","            nn_prediction = nn_model(X)\n","            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n","            nn_loss += criterion(nn_prediction, Y)\n","            nn_acc += nn_correct_prediction.float().mean()\n","\n","        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / test_total_batch, nn_loss / test_total_batch, bn_acc / test_total_batch, nn_acc / test_total_batch\n","\n","        # Save valid losses/acc\n","        valid_losses.append([bn_loss, nn_loss])\n","        valid_accs.append([bn_acc, nn_acc])\n","        print(\n","            '[Epoch %d-VALID] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n","                (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\n","        print()\n","\n","print('Learning finished')"],"metadata":{"id":"X-_D8Px_dNi0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_compare(loss_list: list, ylim=None, title=None) -> None:\n","    bn = [i[0] for i in loss_list]\n","    nn = [i[1] for i in loss_list]\n","\n","    plt.figure(figsize=(15, 10))\n","    plt.plot(bn, label='With BN')\n","    plt.plot(nn, label='Without BN')\n","    if ylim:\n","        plt.ylim(ylim)\n","\n","    if title:\n","        plt.title(title)\n","    plt.legend()\n","    plt.grid('on')\n","    plt.show()"],"metadata":{"id":"4IxyCgG-dNnF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_compare(train_losses, title='Training Loss at Epoch')\n","plot_compare(train_accs, [0, 1.0], title='Training Acc at Epoch')\n","plot_compare(valid_losses, title='Validation Loss at Epoch')\n","plot_compare(valid_accs, [0, 1.0], title='Validation Acc at Epoch')"],"metadata":{"id":"gG0JJxdleiAl"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOPS2VoFM8PHo1W20kwOBdf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 이론 문제\n","\n","** 각 1점 [총 10점] 모두 O,X 문제 입니다. 10번 문제는 첨부한 수식을 참고해 풀어주세요. **\n","\n","\n","1. [1점] nn.Linear(a,b)의 a는 입력 차원의 수, b는 출력 차원의 수입니다. -> O\n","2. [1점] 입력차원이 3개라면 3개의 가중치와 3개의 편향이 필요합니다. -> X (편향은 1개)\n","3. [1점] optimizer.step()은 기존의 기울기(gradient) 값을 초기화하는 단계에 사용되는 함수이다. -> X (개선하는 단계)\n","4. [1점] Minibatch Gradient Descent는 전체 데이터를 균일하게 나누어서, 나눈 각각의 데이터를 하나하나 학습시키는 경사하강법을 의미한다. -> O\n","5. [1점] MSE는 오차 (실제값 - 예측값)들의 합이다. -> X (제곱합)\n","6. [1점] 학습하고자 하는 데이터의 양이 지나치게 방대한 경우 모든 데이터를 학습할 수는 없다. 이를 해결하고자 minibatch로 데이터를 균일하게 나누어 학습하는데 기존의 Batch gradient descent는 매끄럽게 감소하는 반면 minibatch gradient descent는 다소 거칠게 감소한다. -> O\n","7. [1점] optimizer = optim.SGD([W, b], lr=0.01) 에서 lr(learning rate)을 크게 설정할수록 W값을 더욱 빠르고 정확하게 찾을 수 있으므로 되도록 크게 설정하는것이 좋다. -> X (발산할지도..)\n","8. [1점] 다중 선형 회귀에서 x들의 수(샘플의 수 × 특성의 수)가 3X7의 경우 가중치를 W = torch.zeros((3, 1), requires_grad=True)로 선언해야 한다. -> X (7X1)\n","9. [1점] 보통 Training Loss가 더이상 낮아지지 않을 때 Early Stopping을 하여 Overfitting을 방지한다. -> O\n","10. [1점] 위 수식에 의하면 hypothesis[0] = log P(x=1;W), (1 – hypothesis[0]) = log P(x=0;W)이다. -> O\n","    >\n","    (위의 수식 시그마(i=1부터 m) 중 i = 1인 경우에 해당합니다.)\n","    >\n","    (y_train[0] * torch.log(hypothesis[0]) + (1 – y_train[0]) * torch.log(1 – hypothesis[0]))"],"metadata":{"id":"vENsA2Rdlmv5"}},{"cell_type":"markdown","source":["# 실습문제1 [총 5점]\n","#### 출제자: 김시현 # 검수자: 나머지\n","#### 위의 수식을 참고하세요.\n","\n","\n","```\n","x_train = torch.FloatTensor([[88],[95],[80],[100],[74]])\n","y_train = torch.FloatTensor([[80],[94],[85],[99],[73]])\n","# 모델 초기화\n","W = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=0.01)\n","\n","nb_epochs = 100\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    hypothesis = x_train*W+ b\n","\n","    # cost 계산\n","    cost = torch.mean((hypothesis - y_train) ** 2)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","```\n","\n","\n","(1) [2점] 위 코드는 optimizer를 사용하여 경사하강법 구현한 것이다.\n","optimizer를 사용하지 않고 위 코드를 변형하여 경사하강법을 구현하시오.\n","    (hint) 위에 있는 이미지에 있는 식을 활용하여 구현하시오.\n","\n","(2) [3점] 위 코드를 nn.Module로 더욱 간단히 구현해 보시오."],"metadata":{"id":"h9sjddRhmEtH"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"wml3opqalMDW","executionInfo":{"status":"ok","timestamp":1695029551721,"user_tz":-540,"elapsed":5524,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn             # 학습 모델 저장 모듈\n","import torch.nn.functional as F   # 손실함수 저장 모듈\n","import torch.optim as optim       # 최적화 함수 모듈"]},{"cell_type":"markdown","source":["### (1)"],"metadata":{"id":"lkkSJghU3Iwg"}},{"cell_type":"code","source":["# 데이터 준비\n","x_train = torch.FloatTensor([[88],[95],[80],[100],[74]])\n","y_train = torch.FloatTensor([[80],[94],[85],[99],[73]])\n","\n","# 모델 초기화\n","W = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","lr=0.01\n","\n","# 학습\n","nb_epochs = 100\n","for epoch in range(nb_epochs + 1):\n","    # H(x) 계산\n","    hypothesis = x_train*W+ b\n","\n","    # cost 계산\n","    cost = torch.mean((hypothesis - y_train) ** 2)\n","    gradient = torch.sum((W * x_train - y_train) * x_train)\n","\n","    # H(x) 개선\n","    W = W - lr * gradient"],"metadata":{"id":"mtnAhi7flYCE","executionInfo":{"status":"ok","timestamp":1695033325418,"user_tz":-540,"elapsed":310,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["### (2)"],"metadata":{"id":"OFc6gAFU3Oyw"}},{"cell_type":"code","source":["# 모델 생성\n","class LinearRegressionModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Linear(1, 1)\n","\n","    def forward(self, x):\n","        return self.linear(x)"],"metadata":{"id":"jKuhkoVG3YTA","executionInfo":{"status":"ok","timestamp":1695033539282,"user_tz":-540,"elapsed":304,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# 데이터 준비\n","x_train = torch.FloatTensor([[88],[95],[80],[100],[74]])\n","y_train = torch.FloatTensor([[80],[94],[85],[99],[73]])\n","\n","# 모델 선언\n","model = LinearRegressionModel()\n","\n","# 모델 초기화\n","W = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=0.01)\n","\n","# 학습\n","nb_epochs = 100\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    hypothesis = x_train*W+ b\n","\n","    # cost 계산\n","    cost = torch.mean((hypothesis - y_train) ** 2)\n","\n","    # H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()"],"metadata":{"id":"QcuKQJs5lYLe","executionInfo":{"status":"ok","timestamp":1695033542227,"user_tz":-540,"elapsed":424,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# 실습문제2 [총 10점]\n","#### 출제자: 송수연  검수자: 나머지\n","\n","1. [3점] 아래에 주어진 데이터 x_data를 이용하여 내부 배열의 합이 홀수인 경우 1, 짝수인 경우 0으로 라벨링한 y_data를 만드시오. (배열의 합은 코드에서 자동으로 계산되게 하시오.)\n","\t\tx_data = [[1, 2], [2, 6], [3, 1], [3, 4], [5, 3], [6, 2], [5, 4], [2, 1]]\n","\n","2. [2점] x_data와 y_data를 float자료형인 텐서 x_train, y_train로 변경하시오.(이때 x_data의 텐서는 [8,2], y_data의 텐서는 [8,1] 차원을 갖게 하시오.)\n","\n","3. [3점] BinaryClassifier를 수행하는 클래스를 선언하시오. (이때, 입력 차원은 2, 출력 차원은 1이 되게 하시오.)\n","\n","4. [2점] 아래에 주어진 변수값을 대입하여 100마다, cost를 출력하시오. (nn.Module, nn.Linear, nn.Sigmoid를 이용하시오.)\n","\t\tepochs = 1000\n","\t\tlearning_rate = 0.01\n","\n","    (5강 슬라이드 참조)"],"metadata":{"id":"Y_wU_0I0mVgj"}},{"cell_type":"markdown","source":["### (1)"],"metadata":{"id":"ne5wzV3XuOlz"}},{"cell_type":"code","source":["x_data = [[1, 2], [2, 6], [3, 1], [3, 4], [5, 3], [6, 2], [5, 4], [2, 1]]"],"metadata":{"id":"asMr2yS2lYO8","executionInfo":{"status":"ok","timestamp":1695033768434,"user_tz":-540,"elapsed":455,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["y_data = [[(x_data[i][0] + x_data[i][1]) % 2] for i in range(len(x_data))]\n","print(y_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fIgZtU20lYUG","executionInfo":{"status":"ok","timestamp":1695033769711,"user_tz":-540,"elapsed":12,"user":{"displayName":"고은경","userId":"10765015356207281747"}},"outputId":"958cf85b-d6ea-4b2a-8a64-09cd8891a1ed"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1], [0], [0], [1], [0], [0], [1], [1]]\n"]}]},{"cell_type":"markdown","source":["### (2)"],"metadata":{"id":"LjEal3xZuRQf"}},{"cell_type":"code","source":["x_train = torch.FloatTensor(x_data)\n","y_train = torch.FloatTensor(y_data)\n","\n","print(x_train.shape)\n","print(y_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CKNVlnlHlYaM","executionInfo":{"status":"ok","timestamp":1695033771552,"user_tz":-540,"elapsed":393,"user":{"displayName":"고은경","userId":"10765015356207281747"}},"outputId":"db4b4373-bcf4-4e46-ef6a-c2c255147923"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 2])\n","torch.Size([8, 1])\n"]}]},{"cell_type":"markdown","source":["### (3)"],"metadata":{"id":"dLm7TH3duqCj"}},{"cell_type":"code","source":["class BinaryClassifier(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Linear(2, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        return self.sigmoid(self.linear(x))"],"metadata":{"id":"V3fAz3NIuCku","executionInfo":{"status":"ok","timestamp":1695033795368,"user_tz":-540,"elapsed":409,"user":{"displayName":"고은경","userId":"10765015356207281747"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["print(BinaryClassifier().__init__)\n","print(BinaryClassifier().forward(x_train))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UlcoEBf75ZXP","executionInfo":{"status":"ok","timestamp":1695035056445,"user_tz":-540,"elapsed":439,"user":{"displayName":"고은경","userId":"10765015356207281747"}},"outputId":"266b2429-171c-404e-b898-afda6d218c10"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["<bound method BinaryClassifier.__init__ of BinaryClassifier(\n","  (linear): Linear(in_features=2, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")>\n","tensor([[0.6667],\n","        [0.6139],\n","        [0.9014],\n","        [0.8254],\n","        [0.9558],\n","        [0.9810],\n","        [0.9455],\n","        [0.8268]], grad_fn=<SigmoidBackward0>)\n"]}]},{"cell_type":"markdown","source":["### (4)"],"metadata":{"id":"ac8l0xjwutzw"}},{"cell_type":"code","source":["# 모델 준비\n","model = BinaryClassifier()\n","\n","# optimizer 설정\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","# 학습\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    hypothesis = model(x_train)\n","\n","    # cost 계산\n","    cost = F.binary_cross_entropy(hypothesis, y_train)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, cost.item(),\n","        ))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-cnTt51rutTw","executionInfo":{"status":"ok","timestamp":1695033829943,"user_tz":-540,"elapsed":797,"user":{"displayName":"고은경","userId":"10765015356207281747"}},"outputId":"d133e8b0-f97e-492d-de2c-841502d79ad7"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/1000 Cost: 0.840985\n","Epoch  100/1000 Cost: 0.709105\n","Epoch  200/1000 Cost: 0.687606\n","Epoch  300/1000 Cost: 0.677491\n","Epoch  400/1000 Cost: 0.672026\n","Epoch  500/1000 Cost: 0.668297\n","Epoch  600/1000 Cost: 0.665244\n","Epoch  700/1000 Cost: 0.662508\n","Epoch  800/1000 Cost: 0.659970\n","Epoch  900/1000 Cost: 0.657587\n","Epoch 1000/1000 Cost: 0.655342\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Ix8Hwu9x4adm"},"execution_count":null,"outputs":[]}]}